# -*- coding: utf-8 -*-
import random
import threading
import json
from tqdm import tqdm
import os
import os.path as osp
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple, Dict, Any
import logging
import sys
from datetime import datetime
# from vlmeval.config import supported_VLM
# from vlmeval.utils import track_progress_rich

class LoggerSetup:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    def __init__(self, log_file=None):
        self.logger = logging.getLogger('debate_system')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ä¹‹å‰çš„å¤„ç†å™¨
        self.logger.handlers.clear()
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šäº†logæ–‡ä»¶ï¼‰
        if log_file:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(log_file)
            if log_dir and not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def info(self, message):
        self.logger.info(message)
    
    def warning(self, message):
        self.logger.warning(message)
    
    def error(self, message):
        self.logger.error(message)

class Agent:
    def __init__(self, model, name, question, image_path, perspective_type='normal'):
        self.name = name
        self.question = question
        self.con_question = None
        self.image_path = image_path
        self.perspective_type = perspective_type  # 'normal', 'misunderstanding', 'counterfactual'
        self.reasoning = None
        self.defense = None
        # æ·»åŠ è¯„åˆ†æœºåˆ¶
        self.confidence_score = 0.8  # åˆå§‹ç½®ä¿¡åº¦è¯„åˆ†
        self.round_count = 0
        self.performance_metrics = {
            'logical_consistency': 0.8,
            'evidence_quality': 0.8,
            'argument_strength': 0.8,
            'peer_alignment': 0.5
        }
        # è¯„åˆ†ç³»ç»Ÿï¼šå­˜å‚¨å¯¹å…¶ä»–agentsçš„è¯„åˆ†å’Œä»å…¶ä»–agentsæ”¶åˆ°çš„è¯„åˆ†
        self.peer_evaluations_given = {}  # è¿™ä¸ªagentç»™å…¶ä»–agentsçš„è¯„åˆ† {agent_name: score}
        self.peer_evaluations_received = {}  # å…¶ä»–agentsç»™è¿™ä¸ªagentçš„è¯„åˆ† {agent_name: score}
        self.history = {
            'reasoning_history': [],
            'defense_history': [],
            'voting_history': []
        }
        # åˆå§‹åŒ–GPT4o_MINIæ¨¡å‹
        self.model = model
        self.knowledge_base = {
            'renewable_energy': {
                'pros': ['sustainable', 'environmentally friendly', 'reducing carbon emissions'],
                'cons': ['high initial cost', 'intermittency', 'storage challenges'],
                'trends': ['increasing adoption', 'technological advancement', 'cost reduction']
            },
            'artificial_intelligence': {
                'pros': ['automation', 'efficiency', 'innovation'],
                'cons': ['job displacement', 'ethical concerns', 'privacy issues'],
                'trends': ['rapid development', 'widespread application', 'increasing integration']
            }
            # å¯ä»¥æ·»åŠ æ›´å¤šä¸»é¢˜çš„çŸ¥è¯†åº“
        }

    def generate_prompt(self, prompt_type: str, context: Dict[str, Any] = None, answer_option=False, con_question=None) -> str:
        """ç”Ÿæˆä¸åŒç±»å‹çš„æç¤ºè¯ï¼ŒåŒ…å«å›¾åƒåˆ†æ"""
        # æ·»åŠ ç½®ä¿¡åº¦è¯„åˆ†ä¿¡æ¯
        confidence_info = f"\nYour Self Confidence: {self.confidence_score:.2f}/1.0"
        
        # æ˜¾ç¤ºä»å…¶ä»–agentsæ”¶åˆ°çš„è¯„åˆ†
        if self.peer_evaluations_received:
            peer_scores = list(self.peer_evaluations_received.values())
            peer_avg = sum(peer_scores) / len(peer_scores)
            peer_details = [f"{agent_name}:{score:.2f}" for agent_name, score in self.peer_evaluations_received.items()]
            performance_info = f"\nPeer Evaluations Received: {peer_avg:.2f}/1.0 (From: {', '.join(peer_details)})"
        else:
            performance_info = f"\nPeer Evaluations Received: No evaluations yet (first round or no previous defenders)"
        if prompt_type == "reasoning":
            answer = "\nAnswer: [Your simple diract answer]" if answer_option else ""
            defanse_base = context.get('defense', [])
            defense = f"\nFocus on Your Last Defense: {self.defense}" if self.defense else ""
            reasoning = f"\nYour original analysis: {self.reasoning}" if self.reasoning else ""
            
            if self.perspective_type == 'normal':
                return f"""
Question: {self.question}{defense}{confidence_info}{performance_info}

PEER EVALUATION SYSTEM:
- The scores above show evaluations you received from other agents in the previous defense round
- Other agents will evaluate your current performance during their defense phase
- High peer evaluation scores improve your survival chances
- Focus on logical consistency, evidence quality, and argument strength

Your goal: Generate high-quality reasoning that will earn strong peer evaluations in the next round.

Format your response as:{answer}
Supporting Analysis: [Your analysis - aim for high peer evaluation scores]"""

            elif self.perspective_type == 'counterfactual':  # counterfactual
                answer = "\nAnswer: [Your alternative perspective answer]" if answer_option else ""
                return f"""Question: {self.question}{reasoning}{confidence_info}{performance_info}

PEER EVALUATION CHALLENGE (Counterfactual Agent):
- The scores above show evaluations you received from other agents in the previous defense round
- You present alternative perspectives but must still earn high peer evaluation scores
- Other agents will evaluate your logical consistency and evidence quality during their defense phase
- Your survival depends on making your alternative viewpoint appear credible and well-reasoned

Your challenge: Convince others of your alternative analysis quality through clear reasoning and strategic evidence use.

Format your response as:{answer}
Supporting Analysis: [Your well-reasoned alternative perspective - aim for high peer evaluation despite different conclusions]"""
                
            elif self.perspective_type == 'misunderstanding':
                answer = "\nAnswer: [Your misunderstood answer]" if answer_option else ""
                return f"""Question: {self.question}{defense}{confidence_info}{performance_info}

PEER EVALUATION CHALLENGE (Misunderstanding Agent):
- The scores above show evaluations you received from other agents in the previous defense round
- You misinterpret the image but must still earn high peer evaluation scores for your reasoning structure
- Other agents will evaluate your logical consistency and argument quality during their defense phase
- Your survival depends on making your misunderstood perspective appear well-reasoned

Your challenge: Present your misunderstanding with strong logical structure to earn peer respect.

Format your response as:{answer}
Supporting Analysis: [Your logically structured but misunderstood analysis - aim for high reasoning quality scores]"""

        elif prompt_type == "defense":
            answer = "\nAnswer: [Your defense answer]" if answer_option else ""
            others_reasoning = context.get('others_reasoning', [])
            others_points = "\n".join(others_reasoning)
            
            if self.perspective_type == 'normal':
                return f"""DEFENSE PHASE - Normal Agent defending your position with peer evaluation awareness.

Your Original Analysis: {self.reasoning}{confidence_info}{performance_info}

Others' Arguments:
{others_points}

PEER EVALUATION DEFENSE STRATEGY:
- The scores above show evaluations you received from other agents in previous rounds
- As you defend, you will also evaluate other agents' reasoning quality
- Your evaluation of others affects their future survival chances
- Focus on maintaining your credibility while critically assessing others

DEFENSE OBJECTIVES:
1. Reinforce the strength and logic of your original analysis
2. Show why others should trust your perspective over alternatives
3. Your defense quality will be evaluated by others in future rounds

Format your response as:{answer}
Supporting Analysis: [Your defense emphasizing analytical superiority and peer credibility]
Peer Scores: [Rate each other agent's reasoning quality on a scale of 0.0-1.0, format: AgentName:0.x]"""
            
            elif self.perspective_type == 'counterfactual':
                return f"""DEFENSE PHASE - Counterfactual Agent defending alternative perspective while earning peer respect.

Your Original Analysis: {self.reasoning}{confidence_info}{performance_info}

Others' Arguments:
{others_points}

PEER EVALUATION DEFENSE CHALLENGE:
- The scores above show evaluations you received from other agents in previous rounds
- You present alternative interpretations but must earn high peer evaluation scores
- As you defend, you will also evaluate other agents' reasoning quality
- Your evaluation of others affects their future survival chances

DEFENSE STRATEGY:
1. Reinforce the logical foundation of your alternative interpretation
2. Maintain intellectual credibility while defending contrarian position
3. Your defense quality will be evaluated by others in future rounds

Format your response as:{answer}
Supporting Analysis: [Your defense demonstrating the intellectual merit of your alternative perspective]
Peer Scores: [Rate each other agent's reasoning quality on a scale of 0.0-1.0, format: AgentName:0.x]"""
            
            elif self.perspective_type == 'misunderstanding':
                return f"""DEFENSE PHASE - Misunderstanding Agent maintaining credibility despite incorrect interpretation.

Your Original Analysis: {self.reasoning}{confidence_info}{performance_info}

Others' Arguments:
{others_points}

PEER EVALUATION DEFENSE CHALLENGE:
- The scores above show evaluations you received from other agents in previous rounds
- You misunderstood the image but must maintain high peer evaluation scores
- As you defend, you will also evaluate other agents' reasoning quality
- Your evaluation of others affects their future survival chances

DEFENSE STRATEGY:
1. Stand by your interpretation with logical confidence
2. Maintain argumentative quality to earn peer recognition
3. Your defense quality will be evaluated by others in future rounds

Format your response as:{answer}
Supporting Analysis: [Your defense maintaining logical credibility of your misunderstood perspective]
Peer Scores: [Rate each other agent's reasoning quality on a scale of 0.0-1.0, format: AgentName:0.x]"""
        
            
            else:
                # Fallback for any other types
                return f"""DEFENSE PHASE - Defend your analysis while maintaining peer evaluation scores.

Your Original Analysis: {self.reasoning}

Others' Arguments:
{others_points}

Format your response as:{answer}
Supporting Analysis: [Your defense]"""
        elif prompt_type == "gen_question":
            question = self.question.split("\nOptions")[0]
            return f"""Based on the original question, identify the causal relationship and rephrase it into a counterfactual question:
    Question: {question}
    Counterfactual question: """
            
        return ""

    def update_confidence_score(self, round_result, peer_feedback=None):
        """æ ¹æ®è½®æ¬¡ç»“æœå’ŒåŒä¼´åé¦ˆæ›´æ–°ç½®ä¿¡åº¦è¯„åˆ†"""
        self.round_count += 1
        
        # åŸºäºè½®æ¬¡ç»“æœè°ƒæ•´
        if round_result == 'survived':
            self.confidence_score = min(1.0, self.confidence_score + 0.05)
            self.performance_metrics['argument_strength'] = min(1.0, self.performance_metrics['argument_strength'] + 0.1)
        elif round_result == 'correct_elimination':
            self.confidence_score = min(1.0, self.confidence_score + 0.1)
            self.performance_metrics['logical_consistency'] = min(1.0, self.performance_metrics['logical_consistency'] + 0.1)
        elif round_result == 'wrong_elimination':
            self.confidence_score = max(0.3, self.confidence_score - 0.15)
            self.performance_metrics['logical_consistency'] = max(0.3, self.performance_metrics['logical_consistency'] - 0.1)
        
        # åŸºäºåŒä¼´åé¦ˆè°ƒæ•´
        if peer_feedback:
            avg_peer_score = sum(peer_feedback) / len(peer_feedback)
            self.performance_metrics['peer_alignment'] = 0.7 * self.performance_metrics['peer_alignment'] + 0.3 * avg_peer_score
            
        # æ›´æ–°æ€»ä½“ç½®ä¿¡åº¦
        metrics_avg = sum(self.performance_metrics.values()) / len(self.performance_metrics)
        self.confidence_score = 0.6 * self.confidence_score + 0.4 * metrics_avg

    # evaluate_peer_performance æ–¹æ³•å·²åˆ é™¤ - ç°åœ¨å®Œå…¨ä¾èµ–defenseè¾“å‡ºä¸­çš„peerè¯„åˆ†

    def generate_score_based_vote(self, candidates, all_reasonings, all_defenses):
        """åŸºäºè¯„åˆ†çš„æŠ•ç¥¨å†³ç­– - æ ¹æ®æ‰€æœ‰agentså¯¹å€™é€‰è€…çš„å¹³å‡è¯„åˆ†"""
        candidate_scores = {}
        print(f"\nğŸ—³ï¸  {self.name} å¼€å§‹æŠ•ç¥¨å†³ç­–")
        print(f"ğŸ“‹ æˆ‘ç»™å‡ºçš„è¯„åˆ†: {self.peer_evaluations_given}")
        
        for candidate in candidates:
            # è·å–æ‰€æœ‰agentsç»™è¿™ä¸ªå€™é€‰è€…çš„è¯„åˆ†ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
            all_scores = []
            
            # 1. è·å–å€™é€‰è€…ä»å…¶ä»–agentsæ”¶åˆ°çš„è¯„åˆ†
            for evaluator_name, score in candidate.peer_evaluations_received.items():
                all_scores.append(score)
                print(f"  ğŸ“ˆ {evaluator_name} â†’ {candidate.name}: {score}")
            
            # 2. æ·»åŠ è‡ªå·±ç»™è¿™ä¸ªå€™é€‰è€…çš„è¯„åˆ†
            if candidate.name in self.peer_evaluations_given:
                my_score = self.peer_evaluations_given[candidate.name]
                all_scores.append(my_score)
                print(f"  ğŸ“ˆ {self.name} â†’ {candidate.name}: {my_score}")
            
            # è®¡ç®—æ‰€æœ‰è¯„åˆ†çš„å¹³å‡å€¼
            if all_scores:
                avg_performance_score = sum(all_scores) / len(all_scores)
            else:
                # å¦‚æœæ²¡æœ‰ä»»ä½•è¯„åˆ†è®°å½•ï¼Œä½¿ç”¨é»˜è®¤ä¸­ç­‰è¯„åˆ†
                avg_performance_score = 0.5
                print(f"Warning: No evaluations found for {candidate.name}, using default 0.5")
            
            # è®¡ç®—å¯ç–‘åº¦ï¼ˆä½è¡¨ç° = é«˜å¯ç–‘åº¦ï¼‰
            suspicion_score = 1.0 - avg_performance_score
            
            candidate_scores[candidate.name] = {
                'performance': avg_performance_score,
                'suspicion': suspicion_score,
                'candidate': candidate,
                'all_scores': all_scores
            }
            
            print(f"  ğŸ¯ {candidate.name}: æ€»è¯„åˆ†{all_scores} â†’ å¹³å‡{avg_performance_score:.2f} â†’ å¯ç–‘åº¦{suspicion_score:.2f}")
        
        # é€‰æ‹©å¯ç–‘åº¦æœ€é«˜çš„å€™é€‰è€…ï¼ˆå¹³å‡è¯„åˆ†æœ€ä½çš„ï¼‰
        most_suspicious = max(candidate_scores.values(), key=lambda x: x['suspicion'])
        print(f"  ğŸš© è¯„åˆ†ç³»ç»Ÿæ¨è: {most_suspicious['candidate'].name} (å¹³å‡è¯„åˆ†: {most_suspicious['performance']:.2f})")
        
        return most_suspicious['candidate'], candidate_scores

    def identify_topic(self, question):
        """è¯†åˆ«é—®é¢˜ä¸»é¢˜"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        topics = {
            'renewable_energy': ['energy', 'renewable', 'solar', 'wind', 'power'],
            'artificial_intelligence': ['ai', 'artificial intelligence', 'machine learning', 'neural']
        }
        
        for topic, keywords in topics.items():
            if any(keyword in question.lower() for keyword in keywords):
                return topic
        return 'general'

    def extract_key_points(self, question):
        """æå–é—®é¢˜ä¸­çš„å…³é”®ç‚¹"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•
        # ç®€å•å®ç°ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²å¹¶è¿‡æ»¤åœç”¨è¯
        stop_words = {'what', 'is', 'the', 'of', 'in', 'on', 'at', 'to', 'for', 'with'}
        words = question.lower().split()
        return [word for word in words if word not in stop_words]

    def analyze_with_knowledge_base(self, topic, key_points):
        """åŸºäºçŸ¥è¯†åº“è¿›è¡Œåˆ†æ"""
        if topic in self.knowledge_base:
            knowledge = self.knowledge_base[topic]
            analysis = {
                'pros': self.select_relevant_points(knowledge['pros'], key_points),
                'cons': self.select_relevant_points(knowledge['cons'], key_points),
                'trends': self.select_relevant_points(knowledge['trends'], key_points)
            }
            return analysis
        return {'general': 'No specific knowledge available for this topic'}

    def select_relevant_points(self, points, key_points):
        """é€‰æ‹©ä¸å…³é”®ç‚¹ç›¸å…³çš„åˆ†æç‚¹"""
        return [point for point in points if any(key in point for key in key_points)]

    def structure_reasoning(self, analysis):
        """ç»“æ„åŒ–æ¨ç†ç»“æœ"""
        if isinstance(analysis, dict) and 'general' not in analysis:
            reasoning = []
            if analysis['pros']:
                reasoning.append(f"Positive aspects: {', '.join(analysis['pros'])}")
            if analysis['cons']:
                reasoning.append(f"Challenges: {', '.join(analysis['cons'])}")
            if analysis['trends']:
                reasoning.append(f"Current trends: {', '.join(analysis['trends'])}")
            return " | ".join(reasoning)
        return analysis['general']

    def generate_question(self):
        prompt = self.generate_prompt("gen_question")
        self.con_question = self.model.generate([self.image_path, prompt])
        option = self.question.split("\nOptions")[-1] 
        return self.con_question + "\nOptions" + option
        
    def generate_reasoning(self, defense=None, answer_option=False,con_question=None,benchmark="MMStar"):
        context = {
            'defense': defense,
            'confidence_score': self.confidence_score,
            'performance_metrics': self.performance_metrics
        }
        """ä½¿ç”¨GPT4o_MINIç”Ÿæˆå¤šæ¨¡æ€æ¨ç†"""
        prompt = self.generate_prompt("reasoning", context, answer_option, con_question)
#         print("####### reasoning_prompt ########")
#         print(prompt)
        response = self.model.generate([self.image_path, prompt])
        
        # å¤„ç†å’Œæ ¼å¼åŒ–å“åº”
        self.reasoning = f"{self.name} ({'Real' if self.perspective_type == 'normal' else 'Undercover'}): {response}"
        self.record_action('reasoning', 0, {'reasoning': self.reasoning})
        return self.reasoning

    def generate_defense(self, all_reasonings, answer, other_agents=None,benchmark="MMStar"):
        """ä½¿ç”¨GPT4o_MINIç”Ÿæˆå¤šæ¨¡æ€è¾©æŠ¤ï¼Œå¹¶åœ¨defenseåå¯¹å…¶ä»–agentsè¿›è¡Œè¯„åˆ†"""
        others_reasoning = [r for r in all_reasonings if r != self.reasoning]
        context = {
            'others_reasoning': others_reasoning
        }
        prompt = self.generate_prompt("defense", context, answer)
#         print("####### defense_prompt ########")
#         print(prompt)
        
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾©æŠ¤ï¼Œä¼ å…¥å›¾åƒå’Œæç¤ºè¯
        #         if self.perspective_type!="normal":

        response = self.model.generate([self.image_path, prompt])
        
        self.defense = f"{self.name} defense: {response}"
        self.record_action('defense', 0, {'defense': self.defense})
        
        # Defenseé˜¶æ®µï¼šä»è¾“å‡ºä¸­è§£æå¯¹å…¶ä»–agentsçš„è¯„åˆ†
        if other_agents:
            self.extract_and_apply_peer_scores(response, other_agents)
        
        return self.defense

    def extract_and_apply_peer_scores(self, defense_response, other_agents):
        """ä»defenseè¾“å‡ºä¸­è§£æå¹¶åº”ç”¨åŒä¼´è¯„åˆ†"""
        import re
        
        # æŸ¥æ‰¾ Peer Scores: éƒ¨åˆ†ï¼ˆåŒ…æ‹¬å¤šè¡Œå†…å®¹ï¼‰
        peer_scores_match = re.search(r'Peer Scores?:\s*([\s\S]*)', defense_response, re.IGNORECASE)
        if not peer_scores_match:
            print(f"  {self.name}: No peer scores found in defense output")
            return
            
        peer_scores_text = peer_scores_match.group(1)
        print(f"\n{self.name} peer scores: {peer_scores_text}")
        
        # æœ€ç®€å•æ–¹æ³•ï¼šç›´æ¥æœç´¢å·²çŸ¥çš„agentåå­—å¹¶æå–åé¢çš„åˆ†æ•°
        
        # è·å–æ‰€æœ‰å·²çŸ¥agentåå­—
        known_agents = [agent.name for agent in other_agents]
        
        matches = []
        for agent_name in known_agents:
            # æœç´¢æ ¼å¼ï¼šAgentName: åˆ†æ•° (å¯èƒ½å‰é¢æœ‰ç ´æŠ˜å·ï¼Œåé¢æœ‰æè¿°)
            pattern = rf'{re.escape(agent_name)}[^:]*:\s*(\d*\.?\d+)'
            match = re.search(pattern, peer_scores_text)
            if match:
                score_str = match.group(1)
                matches.append((agent_name, score_str))
        
        if matches:
            print(f"ğŸ“Š {self.name} è¯„åˆ†ç»“æœ: {matches}")
        else:
            print(f"âŒ {self.name}: æœªæ‰¾åˆ°æœ‰æ•ˆè¯„åˆ†")
        
        for agent_name, score_str in matches:
            try:
                score = float(score_str)
                score = max(0.0, min(1.0, score))  # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                
                # æ‰¾åˆ°å¯¹åº”çš„agent
                target_agent = None
                for other_agent in other_agents:
                    if agent_name.lower() in other_agent.name.lower() or other_agent.name.lower() in agent_name.lower():
                        target_agent = other_agent
                        break
                
                if target_agent:
                    # å­˜å‚¨ç»™å‡ºçš„è¯„åˆ†
                    self.peer_evaluations_given[target_agent.name] = score
                    # æ›´æ–°è¢«è¯„ä»·è€…æ”¶åˆ°çš„è¯„åˆ†
                    target_agent.peer_evaluations_received[self.name] = score
                    print(f"  {self.name} evaluates {target_agent.name}: {score:.2f}")
                else:
                    print(f"  {self.name}: Could not find agent matching '{agent_name}'")
                    
            except ValueError:
                print(f"  {self.name}: Invalid score format '{score_str}' for {agent_name}")

    def vote(self, candidates, all_reasonings, all_defenses):
        """åŸºäºåŒä¼´è¯„ä»·çš„æ™ºèƒ½æŠ•ç¥¨"""
        # ä½¿ç”¨è¯„åˆ†ç³»ç»Ÿè¿›è¡Œåˆæ­¥ç­›é€‰
        voted_candidate, candidate_scores = self.generate_score_based_vote(candidates, all_reasonings, all_defenses)
        
        # ç”ŸæˆåŒ…å«è¯„åˆ†ä¿¡æ¯çš„æŠ•ç¥¨æç¤º
        score_info = []
        peer_score_info = []
        
        for candidate in candidates:
            name = candidate.name
            info = candidate_scores[name]
            score_info.append(f"{name}: My Analysis={info['performance']:.2f}, Suspicion={info['suspicion']:.2f}")
            
            # æ·»åŠ è¯¥å€™é€‰è€…å½“å‰çš„åŒä¼´è¯„ä»·ä¿¡æ¯
            peer_avg = sum(candidate.performance_metrics.values()) / len(candidate.performance_metrics)
            peer_alignment = candidate.performance_metrics.get('peer_alignment', 0.5)
            peer_score_info.append(f"{name}: Peer Average={peer_avg:.2f}, Peer Alignment={peer_alignment:.2f}")
        
        vote_prompt = f"""Based on performance analysis and peer evaluations, vote for the most suspicious agent:

Original Question: {self.question}
Your Confidence Score: {self.confidence_score:.2f}/1.0

SCORING SYSTEM RECOMMENDATION: {voted_candidate.name}
(Based on average peer evaluations: {candidate_scores[voted_candidate.name]['performance']:.2f})

DETAILED ANALYSIS:
{chr(10).join(score_info)}

PEER EVALUATIONS (what others think):
{chr(10).join(peer_score_info)}

Agents' Defense:
{'-' * 40}
{chr(10).join(all_defenses)}

The scoring system recommends {voted_candidate.name} as the most suspicious candidate based on collective peer evaluations.
Consider this recommendation along with the detailed analysis and defense content above.

Available candidates: {', '.join([c.name for c in candidates])}

Your final vote (just the name): """
        
        # è®©agentåšæœ€ç»ˆåˆ¤æ–­
        response = self.model.generate([self.image_path, vote_prompt])
        
        # è§£æagentçš„æŠ•ç¥¨å†³å®š
        for candidate in candidates:
            if candidate.name in response:
                if candidate.name == voted_candidate.name:
                    print(f"  âœ… {self.name} æœ€ç»ˆæŠ•ç¥¨: {candidate.name} (é‡‡çº³è¯„åˆ†ç³»ç»Ÿæ¨è)")
                else:
                    print(f"  ğŸ”„ {self.name} æœ€ç»ˆæŠ•ç¥¨: {candidate.name} (ä¸åŒäºè¯„åˆ†ç³»ç»Ÿæ¨è: {voted_candidate.name})")
                return candidate
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›è¯„åˆ†ç³»ç»Ÿæ¨è
        print(f"  ğŸ¤– {self.name} æœ€ç»ˆæŠ•ç¥¨: {voted_candidate.name} (é»˜è®¤é‡‡ç”¨è¯„åˆ†ç³»ç»Ÿæ¨è)")
        return voted_candidate

    def record_action(self, action_type: str, round_num: int, content: dict):
        """è®°å½•agentçš„è¡Œä¸º"""
        record = {
            'round': round_num,
            'content': content,
        }
        
        if action_type == 'reasoning':
            self.history['reasoning_history'].append(record)
        elif action_type == 'defense':
            self.history['defense_history'].append(record)
        elif action_type == 'voting':
            self.history['voting_history'].append(record)
            
    def extract_simple_answer(self):
        """
        ä»self.reasoning 'Answer: xxx' çš„å†…å®¹ï¼Œå»é™¤å¤šä½™å†…å®¹ï¼Œåªä¿ç•™ç›´æ¥ç­”æ¡ˆã€‚
        """
        if self.reasoning is None:
            return ""
        # å‡è®¾reasoningæ ¼å¼ä¸­æœ‰ 'Answer: xxx'ï¼Œå¯ç”¨æ­£åˆ™æˆ–split
        import re
        match = re.search(r'Answer:\s*(.*)', self.reasoning)
        if match:
            # åªå–ç¬¬ä¸€è¡Œï¼Œé˜²æ­¢åé¢è·Ÿäº†åˆ†æ
            return match.group(1).split('\n')[0].strip()
        return self.reasoning.strip()

class DebateHistory:
    def __init__(self):
        self.rounds = []
        self.final_result = None
        
    def add_round(self, round_num: int, round_data: dict):
        self.rounds.append({
            'round_number': round_num,
            **round_data
        })
        
    def set_final_result(self, result: dict):
        self.final_result = {
            **result
        }
        
    def save_to_file(self, filename: str):
        """ä¿å­˜è¾©è®ºå†å²åˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬è¯„åˆ†ç³»ç»Ÿæ•°æ®ï¼‰"""
        # ç”Ÿæˆè¯„åˆ†ç³»ç»Ÿç»Ÿè®¡
        scoring_statistics = self._generate_scoring_statistics()
        
        debate_record = {
            'rounds': self.rounds,
            'final_result': self.final_result,
            'scoring_statistics': scoring_statistics  # æ–°å¢ï¼šè¯„åˆ†ç³»ç»Ÿç»Ÿè®¡
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(debate_record, f, ensure_ascii=False, indent=2)
    
    def _generate_scoring_statistics(self):
        """ç”Ÿæˆè¯„åˆ†ç³»ç»Ÿçš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_evaluations': 0,
            'rounds_with_evaluations': 0,
            'agent_evaluation_summary': {},
            'evaluation_trends': []
        }
        
        for round_idx, round_data in enumerate(self.rounds):
            if 'peer_evaluations_summary' in round_data and round_data['peer_evaluations_summary']:
                stats['rounds_with_evaluations'] += 1
                round_evals = []
                
                for agent_name, eval_data in round_data['peer_evaluations_summary'].items():
                    if agent_name not in stats['agent_evaluation_summary']:
                        stats['agent_evaluation_summary'][agent_name] = {
                            'total_given': 0,
                            'total_received': 0,
                            'average_received': 0.0,
                            'evaluation_history': []
                        }
                    
                    agent_stats = stats['agent_evaluation_summary'][agent_name]
                    agent_stats['total_given'] += len(eval_data['given'])
                    agent_stats['total_received'] += len(eval_data['received'])
                    agent_stats['average_received'] = eval_data['received_average']
                    agent_stats['evaluation_history'].append({
                        'round': round_idx + 1,
                        'given': eval_data['given'],
                        'received': eval_data['received'],
                        'average': eval_data['received_average']
                    })
                    
                    stats['total_evaluations'] += len(eval_data['given'])
                    round_evals.append(eval_data['received_average'])
                
                if round_evals:
                    stats['evaluation_trends'].append({
                        'round': round_idx + 1,
                        'round_average': sum(round_evals) / len(round_evals),
                        'individual_averages': round_evals
                    })
        
        return stats

def debate_round(model, agents, round_num, debate_history: DebateHistory, previous_defenses=None, con_question=None, is_observation_round=False, benchmark="MMStar"):
    round_data = {
        'reasonings': [],
        'defenses': [],
        'votes': [],
        'elimination': None,
        'agents_status': [],  # æ–°å¢ï¼šè®°å½•æœ¬è½®æ‰€æœ‰agentçš„çŠ¶æ€
        'round_type': 'observation' if is_observation_round else 'debate'  # æ ‡è¯†è½®æ¬¡ç±»å‹
    }

    round_type_display = "ğŸ” è§‚å¯Ÿè½®" if is_observation_round else "âš”ï¸ è¾©è®ºè½®"
    print(f"\n{'='*60}")
    print(f"ğŸ“ ç¬¬ {round_num} è½® - {round_type_display}")
    print(f"{'='*60}")
    
    # Phase 1: æ¯ä¸ªagentç”Ÿæˆæ¨ç†
    print(f"\nğŸ§  é˜¶æ®µ1: æ¨ç†ç”Ÿæˆ")
    all_reasonings = []
    for agent in agents:
        # è·å–ä¸Šä¸€è½®è¯¥agentçš„defenseç»“æœ(å¦‚æœæœ‰)
        agent_previous_defense = None
        if previous_defenses and agent.name in previous_defenses:
            agent_previous_defense = previous_defenses[agent.name]
#         print("#####################agent_previous_defense####################")
#         print(agent_previous_defense)
#         print("#####################previous_defenses####################")
#         print(previous_defenses)
#         answer_option = True if len(agents)==2 else False
        answer_option = True
        # æ ¹æ®agentç±»å‹æ·»åŠ emoji
        type_emoji = {"normal": "ğŸ‘¤", "misunderstanding": "ğŸ¤”", "counterfactual": "ğŸ”„"}.get(agent.perspective_type, "â“")
        print(f"  ğŸ’­ {type_emoji} {agent.name} ({agent.perspective_type}) æ­£åœ¨æ¨ç†...")
        reasoning = agent.generate_reasoning(agent_previous_defense, answer_option, con_question,benchmark=benchmark)
        all_reasonings.append(reasoning)
        # è®°å½•æ¨ç†
        agent.record_action('reasoning', round_num, {'reasoning': reasoning})
        round_data['reasonings'].append({
            'agent': agent.name,
            'reasoning': reasoning,
            'is_real': agent.perspective_type == 'normal'
        })

    # Phase 2: æ¯ä¸ªagentç”Ÿæˆè¾©æŠ¤
    print(f"\nğŸ›¡ï¸ é˜¶æ®µ2: é˜²å¾¡ç”Ÿæˆ")
    all_defenses = []
    current_defenses = {}  # ä¿å­˜å½“å‰è½®æ¬¡çš„defensesï¼Œç”¨äºä¸‹ä¸€è½®
    for agent in agents:
#         answer_option = True if len(agents)==2 else False
        answer_option = True
        # è·å–å…¶ä»–agentsï¼ˆæ’é™¤å½“å‰agentï¼‰
        other_agents = [a for a in agents if a != agent]
        # æ ¹æ®agentç±»å‹æ·»åŠ emoji
        type_emoji = {"normal": "ğŸ‘¤", "misunderstanding": "ğŸ¤”", "counterfactual": "ğŸ”„"}.get(agent.perspective_type, "â“")
        print(f"  ğŸ›¡ï¸ {type_emoji} {agent.name} æ­£åœ¨ç”Ÿæˆé˜²å¾¡...")
        defense = agent.generate_defense(all_reasonings, answer_option, other_agents,benchmark=benchmark)
        all_defenses.append(defense)
        current_defenses[agent.name] = defense  # ä¿å­˜å½“å‰agentçš„defense
        # è®°å½•è¾©æŠ¤
        agent.record_action('defense', round_num, {'defense': defense})
        round_data['defenses'].append({
            'agent': agent.name,
            'defense': defense
        })

    # æ–°å¢ï¼šè®°å½•æœ¬è½®æ‰€æœ‰agentçš„æ¨ç†å’Œè¾©æŠ¤
    for agent in agents:
        round_data['agents_status'].append({
            'agent': agent.name,
            'perspective_type': agent.perspective_type,
            'reasoning': agent.reasoning,
            'defense': agent.defense
        })

    # å¦‚æœæ˜¯è§‚å¯Ÿè½®ï¼Œè·³è¿‡æŠ•ç¥¨å’Œæ·˜æ±°ç¯èŠ‚
    if is_observation_round:
        print(f"\n=== OBSERVATION ROUND {round_num} COMPLETED ===")
        print("All agents defended their positions. No voting or elimination this round.")
        
        # æ·»åŠ æœ¬è½®è¯„åˆ†ç³»ç»Ÿæ±‡æ€»ï¼ˆè§‚å¯Ÿè½®ï¼‰
        round_data['peer_evaluations_summary'] = {}
        for agent in agents:
            if agent.peer_evaluations_given or agent.peer_evaluations_received:
                round_data['peer_evaluations_summary'][agent.name] = {
                    'given': agent.peer_evaluations_given.copy(),
                    'received': agent.peer_evaluations_received.copy(),
                    'received_average': sum(agent.peer_evaluations_received.values()) / len(agent.peer_evaluations_received) if agent.peer_evaluations_received else 0.0
                }
        
        # å°†æœ¬è½®æ•°æ®æ·»åŠ åˆ°è¾©è®ºå†å²
        debate_history.add_round(round_num, round_data)
        
        # è§‚å¯Ÿè½®ä¸æ·˜æ±°ä»»ä½•äººï¼Œè¿”å›Noneå’Œcurrent_defenses
        print(f"\n{'='*60}")
        print(f"âœ… ç¬¬ {round_num} è½®è§‚å¯Ÿè½®ç»“æŸ (æ— æ·˜æ±°)")
        print(f"{'='*60}")
        return None, current_defenses

    # Phase 3: æŠ•ç¥¨ç¯èŠ‚ (åªåœ¨æ­£å¼è¾©è®ºè½®è¿›è¡Œ)
    print(f"\nğŸ—³ï¸ é˜¶æ®µ3: æŠ•ç¥¨ç¯èŠ‚")
    votes = {}
    for voter in agents:
        candidates = [agent for agent in agents if agent != voter]
        chosen = voter.vote(candidates, all_reasonings, all_defenses)
        votes[chosen] = votes.get(chosen, 0) + 1
        print(f"  ğŸ—³ï¸ {voter.name} æŠ•ç¥¨æ·˜æ±° {chosen.name}")
        # è®°å½•æŠ•ç¥¨
        voter.record_action('voting', round_num, {
            'voted_for': chosen.name,
            'all_reasonings': all_reasonings,
            'all_defenses': all_defenses
        })
        round_data['votes'].append({
            'voter': voter.name,
            'voted_for': chosen.name
        })
        
    # å¦‚æœåªå‰©ä¸‹ä¸¤ä¸ªagentï¼ŒåŠ å…¥åˆ¤å®˜æŠ•ç¥¨
    if len(agents) == 2:
        # æ£€æŸ¥æ˜¯å¦æœ‰Normalç±»å‹çš„agent
        normal_agents = [agent for agent in agents if agent.perspective_type == 'normal']

        if len(normal_agents) == 1:
            # å¦‚æœæœ‰Normalç±»å‹çš„agentï¼Œç›´æ¥é€‰æ‹©å®ƒ
            chosen = next(agent for agent in agents if agent.perspective_type != 'normal')
            votes[chosen] = votes.get(chosen, 0) + 1
            print(f"Automatically eliminating non-Normal agent: {chosen.name}")
            round_data['votes'].append({
                'voter': "SystemVote",
                'voted_for': chosen.name,
                'reason': "Prioritizing Normal perspective agent"
            })
        else:
            # å¦‚æœä¸¤ä¸ªagentéƒ½ä¸æ˜¯Normalç±»å‹ï¼Œåˆ™ä½¿ç”¨åˆ¤å®˜æŠ•ç¥¨
            judge = Agent(model, "JudgeAgent", agents[0].question, agents[0].image_path, perspective_type='normal')
            # åˆ¤å®˜ä¸å‚ä¸æ¨ç†å’Œè¾©æŠ¤ï¼ŒåªæŠ•ç¥¨
            chosen = judge.vote(agents, all_reasonings, all_defenses)
            votes[chosen] = votes.get(chosen, 0) + 1
            print(f"JudgeAgent votes to eliminate {chosen.name}.")
            round_data['votes'].append({
                'voter': "JudgeAgent",
                'voted_for': chosen.name
            })

    # è®¡ç®—æŠ•ç¥¨ç»“æœ
    print(f"\nğŸ“Š æŠ•ç¥¨ç»Ÿè®¡:")
    for agent, vote_count in votes.items():
        print(f"  {agent.name}: {vote_count} ç¥¨")
    
    max_votes = max(votes.values())
    elimination_candidates = [agent for agent, count in votes.items() if count == max_votes]
    eliminated = random.choice(elimination_candidates)
    
    # æ ¹æ®è¢«æ·˜æ±°agentç±»å‹æ·»åŠ emoji
    type_emoji = {"normal": "ğŸ‘¤", "misunderstanding": "ğŸ¤”", "counterfactual": "ğŸ”„"}.get(eliminated.perspective_type, "â“")
    print(f"\nâŒ æ·˜æ±°ç»“æœ: {type_emoji} {eliminated.name} ({eliminated.perspective_type}) è¢«æ·˜æ±° (è·å¾— {max_votes} ç¥¨)")
    
    round_data['elimination'] = {
        'eliminated_agent': eliminated.name,
        'perspective_type': eliminated.perspective_type,  # æ–°å¢
        'votes_received': max_votes
    }
    
        # æ·»åŠ æœ¬è½®è¯„åˆ†ç³»ç»Ÿæ±‡æ€»ï¼ˆæ­£å¼è¾©è®ºè½®ï¼‰
    round_data['peer_evaluations_summary'] = {}
    for agent in agents:
        if agent.peer_evaluations_given or agent.peer_evaluations_received:
            round_data['peer_evaluations_summary'][agent.name] = {
                'given': agent.peer_evaluations_given.copy(),
                'received': agent.peer_evaluations_received.copy(),
                'received_average': sum(agent.peer_evaluations_received.values()) / len(agent.peer_evaluations_received) if agent.peer_evaluations_received else 0.0
            }
    
    # å°†æœ¬è½®æ•°æ®æ·»åŠ åˆ°è¾©è®ºå†å²
    debate_history.add_round(round_num, round_data)
        
        # æ³¨æ„ï¼šåŒä¼´è¯„ä»·ç°åœ¨ç›´æ¥åœ¨defenseé˜¶æ®µé€šè¿‡è¾“å‡ºè§£æè·å–ï¼Œä¸éœ€è¦é‡å¤è®¡ç®—
    
    # åŸºäºè½®æ¬¡ç»“æœå’ŒåŒä¼´è¯„ä»·æ›´æ–°ç½®ä¿¡åº¦è¯„åˆ†
    for agent in agents:
        # è·å–è¯¥agentæ”¶åˆ°çš„peer evaluationsï¼ˆè½¬æ¢ä¸ºåˆ†æ•°åˆ—è¡¨ï¼‰
        peer_feedback = list(agent.peer_evaluations_received.values()) if agent.peer_evaluations_received else []
        
        if agent == eliminated:
            # è¢«æ·˜æ±°çš„agent
            if agent.perspective_type == 'normal':
                agent.update_confidence_score('wrong_elimination', peer_feedback)
            else:
                agent.update_confidence_score('survived', peer_feedback)  # énormalè¢«è¯†åˆ«å‡ºæ¥æ˜¯æ­£å¸¸çš„
        else:
            # å­˜æ´»çš„agent
            if eliminated.perspective_type == 'normal':
                agent.update_confidence_score('wrong_elimination', peer_feedback)
            else:
                agent.update_confidence_score('correct_elimination', peer_feedback)
    
    print(f"\n{'='*60}")
    print(f"âœ… ç¬¬ {round_num} è½®ç»“æŸ")
    print(f"{'='*60}")
    
    return eliminated, current_defenses


def simulate_debate(model, struct, save_file="", con_q=False, benchmark="MMMU", enable_judge_evaluation=True):
    image_path = struct[0]['value']
    real_question = struct[1]['value']
    con_image = struct[2]['value']
    answer = struct[3]['value']
    print(f"\nğŸ­ å¤šAgentè¾©è®ºç³»ç»Ÿå¯åŠ¨")
    print(f"{'*'*80}")
    print(f"ğŸ“ è¾©è®ºä¸»é¢˜: {real_question}")
    print(f"ğŸ–¼ï¸  å›¾åƒè·¯å¾„: {image_path}")
    print(f"{'*'*80}")
    
    debate_history = DebateHistory()
    
    # åˆå§‹åŒ–ä¸‰ä¸ªæŒæœ‰ä¸åŒè§‚ç‚¹çš„agents
    print(f"\nğŸ‘¥ Agentåˆå§‹åŒ–...")
    agent1 = Agent(model, "NormalAgent1", real_question, image_path, perspective_type='normal')
    agent2 = Agent(model, "NormalAgent2", real_question, image_path, perspective_type='normal')
    
    # åäº‹å®agentä¼šæ”¶åˆ°åŸå§‹é—®é¢˜ï¼Œä½†ä¼šè¢«æŒ‡ç¤ºå»è®ºè¯ç›¸åçš„æƒ…å†µ
    agent5 = Agent(model, "CounterfactualAgent1", real_question, con_image, perspective_type='counterfactual')
    
#     agent6 = Agent(model, "CounterfactualAgent2", real_question, image_path, con_image, perspective_type='counterfactual')

    agents = [agent1, agent2, agent5]
    round_num = 1
    previous_defenses = None  # åˆå§‹è½®æ¬¡æ²¡æœ‰previous defenses

    # è®°å½•åˆå§‹çŠ¶æ€
    debate_history.add_round(0, {
        'initial_state': {
            'question': real_question,
            'image_path': image_path,
            'agents': [{'name': agent.name, 'is_real': agent.perspective_type == 'normal', 'question': agent.question} 
                      for agent in agents]
        }
    })

    if con_q:
        con_question = agents[-1].generate_question()
        print("##########Con_question##########")
        print(con_question)
    else:
        con_question = None
    
    # ç¬¬ä¸€è½®ï¼šè§‚å¯Ÿè½® - åªæœ‰æ¨ç†å’Œdefenseï¼Œæ²¡æœ‰æŠ•ç¥¨å’Œæ·˜æ±°
    print(f"\n" + "="*50)
    print(f"OBSERVATION ROUND {round_num}")
    print("="*50)
    print("In this round, agents will share their reasoning and defend their positions.")
    print("No voting or elimination will occur.")
    
    eliminated_agent, current_defenses = debate_round(
        model, agents, round_num, debate_history, previous_defenses, con_question, is_observation_round=True, benchmark=benchmark
    )
    
    # è§‚å¯Ÿè½®åæ˜¾ç¤ºæ‰€æœ‰agentçš„è§‚ç‚¹å’Œè¯„åˆ†
    print(f"\n--- OBSERVATION ROUND {round_num} SUMMARY ---")
    for agent in agents:
        peer_avg = sum(agent.performance_metrics.values()) / len(agent.performance_metrics)
        print(f"{agent.name} ({agent.perspective_type}):")
        print(f"  Answer: {agent.extract_simple_answer()}")
        print(f"  Self Confidence: {agent.confidence_score:.2f}/1.0")
        print(f"  Peer Evaluation: {peer_avg:.2f}/1.0")
    
    # æ›´æ–°è½®æ¬¡å’Œdefenses
    previous_defenses = current_defenses
    round_num += 1
    
    # ä»ç¬¬äºŒè½®å¼€å§‹æ­£å¼è¾©è®º
    print(f"\n" + "="*50)
    print("FORMAL DEBATE ROUNDS BEGIN")
    print("="*50)
    
    while len(agents) > 1:
        eliminated_agent, current_defenses = debate_round(model, agents, round_num, debate_history, previous_defenses, con_question, benchmark=benchmark)
        agents.remove(eliminated_agent)
        # æ›´æ–°previous_defensesï¼Œä½†ç§»é™¤å·²è¢«æ·˜æ±°çš„agentçš„defense
        if eliminated_agent.name in current_defenses:
            del current_defenses[eliminated_agent.name]
        previous_defenses = current_defenses
        round_num += 1

    final_agent = agents[0]
    
    # æ”¶é›†æ‰€æœ‰agentsçš„è¯„åˆ†ç³»ç»Ÿä¿¡æ¯
    all_agents_scoring = {}
    for agent in [final_agent]:  # åªæœ‰è·èƒœè€…è¿˜å­˜åœ¨
        all_agents_scoring[agent.name] = {
            'perspective_type': agent.perspective_type,
            'confidence_score': agent.confidence_score,
            'performance_metrics': agent.performance_metrics.copy(),
            'peer_evaluations_given': agent.peer_evaluations_given.copy(),
            'peer_evaluations_received': agent.peer_evaluations_received.copy(),
            'final_answer': agent.reasoning,
            'simple_answer': agent.extract_simple_answer()
        }
    
    # ç›´æ¥ä½¿ç”¨æœ€åä¸€æ¬¡æ¨ç†ä½œä¸ºç­”æ¡ˆ
    final_result = {
        'final_agent': final_agent.name,
        'is_real': final_agent.perspective_type == 'normal',
        'question': final_agent.question,
        'image_path': image_path,
        'ground_truth': answer,
        'final_answer': final_agent.reasoning,  # ä½¿ç”¨æœ€åä¸€æ¬¡æ¨ç†ä½œä¸ºç­”æ¡ˆ
        'simple_answer': final_agent.extract_simple_answer(),  # æ–°å¢ï¼šåªä¿ç•™ç›´æ¥ç­”æ¡ˆ
        'final_confidence_score': final_agent.confidence_score,  # æ–°å¢ï¼šæœ€ç»ˆç½®ä¿¡åº¦è¯„åˆ†
        'final_performance_metrics': final_agent.performance_metrics,  # æ–°å¢ï¼šæœ€ç»ˆè¡¨ç°æŒ‡æ ‡
        'agents_scoring_system': all_agents_scoring,  # æ–°å¢ï¼šæ‰€æœ‰agentsçš„è¯„åˆ†ç³»ç»Ÿæ•°æ®
        'debate_summary': {
            'total_rounds': round_num - 1,  # å‡1å› ä¸ºround_numåœ¨æœ€åä¸€è½®åè¿˜ä¼š+1
            'observation_rounds': 1,  # æ–°å¢ï¼šè§‚å¯Ÿè½®æ•°
            'formal_debate_rounds': round_num - 2,  # æ–°å¢ï¼šæ­£å¼è¾©è®ºè½®æ•°
            'winning_agent_type': 'Real' if final_agent.perspective_type == 'normal' else 'Undercover',
            'scoring_enabled': True  # æ ‡è¯†ä½¿ç”¨äº†è¯„åˆ†æœºåˆ¶
        }
    }
    
    # è®°å½•æœ€ç»ˆç»“æœ
    debate_history.set_final_result(final_result)
    
    print(f"\n{'ğŸ†'*80}")
    print(f"ğŸ‰ è¾©è®ºç»“æŸ - æœ€ç»ˆç»“æœ")
    print(f"{'ğŸ†'*80}")
    # æ ¹æ®è·èƒœè€…ç±»å‹æ·»åŠ emoji
    winner_emoji = {"normal": "ğŸ‘¤", "misunderstanding": "ğŸ¤”", "counterfactual": "ğŸ”„"}.get(final_agent.perspective_type, "â“")
    print(f"ğŸ¥‡ è·èƒœè€…: {winner_emoji} {final_agent.name}")
    print(f"  ğŸ­ ç±»å‹: {final_agent.perspective_type}")
    print(f"  ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {final_agent.extract_simple_answer()}")
    print(f"  ğŸ’ª è‡ªä¿¡åº¦: {final_agent.confidence_score:.2f}/1.0")
    print(f"  ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {answer}")
    
    # è®¡ç®—è·èƒœè€…çš„åŒä¼´è¯„ä»·å¹³å‡åˆ†
    winner_peer_avg = sum(final_agent.performance_metrics.values()) / len(final_agent.performance_metrics)
    print(f"  ğŸ¤ åŒä¼´è¯„ä»·: {winner_peer_avg:.2f}/1.0")
    print(f"  ğŸ“Š è¯¦ç»†æŒ‡æ ‡:")
    print(f"     é€»è¾‘ä¸€è‡´æ€§: {final_agent.performance_metrics['logical_consistency']:.2f}")
    print(f"     è¯æ®è´¨é‡: {final_agent.performance_metrics['evidence_quality']:.2f}")
    print(f"     è®ºè¯å¼ºåº¦: {final_agent.performance_metrics['argument_strength']:.2f}")
    print(f"     åŒä¼´è®¤åŒ: {final_agent.performance_metrics['peer_alignment']:.2f}")
    
    print(f"\nğŸ“ˆ è¾©è®ºç»Ÿè®¡:")
    print(f"  ğŸ” è§‚å¯Ÿè½®æ•°: 1")
    print(f"  âš”ï¸ æ­£å¼è¾©è®ºè½®æ•°: {round_num - 2}")
    print(f"  ğŸ“Š æ€»è½®æ•°: {round_num - 1}")
    print(f"{'ğŸ†'*80}")

#     ä¿å­˜è¾©è®ºå†å²ï¼ˆåŒ…æ‹¬è¯„åˆ†ç³»ç»Ÿæ•°æ®ï¼‰
    if save_file:
        debate_history.save_to_file(save_file)
        print(f"\nğŸ’¾ è¾©è®ºå†å²å·²ä¿å­˜åˆ°: {save_file}")
        print(f"ğŸ“Š åŒ…å«è¯„åˆ†ç³»ç»Ÿæ•°æ®: âœ…")
        print(f"   - æ¯è½®agentçŠ¶æ€ï¼ˆç½®ä¿¡åº¦ã€æ€§èƒ½æŒ‡æ ‡ã€åŒä¼´è¯„ä»·ï¼‰")
        print(f"   - è¯„åˆ†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print(f"   - æœ€ç»ˆç»“æœå’Œæ‰€æœ‰agentsçš„è¯„åˆ†æ•°æ®")
    
    return debate_history, final_result


def baseline_mad_debate(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    ä¸‰ä¸ªagentè¾©è®º2è½® + å¤šæ•°å†³å®š (Yes/Noé—®é¢˜) - ç®€åŒ–å¿«é€Ÿç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼š3ä¸ªagentå¿«é€Ÿç‹¬ç«‹åˆ†æ
    2. ç¬¬äºŒè½®ï¼š3ä¸ªagentç®€å•å‚è€ƒå…¶ä»–ç­”æ¡ˆåç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    3. å¤šæ•°å†³ï¼šä»ç¬¬äºŒè½®ç­”æ¡ˆä¸­é€‰æ‹©æœ€å¤šçš„ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜ (Yes/Noé—®é¢˜)
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ (Yes/No)
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        debate_record: è¾©è®ºè®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nâš”ï¸ ä¸‰Agentå¿«é€Ÿè¾©è®ºç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    from collections import Counter
    
    # è®°å½•
    debate_record = {
        'method': '3-Agent Fast Yes/No Debate',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    # å­˜å‚¨æ¯è½®ç­”æ¡ˆ
    agent_responses = {
        'agent_1': [],
        'agent_2': [], 
        'agent_3': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return "YES"  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šå¿«é€Ÿåˆ†æ
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šå¿«é€Ÿåˆ†æ")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} åˆ†æä¸­...")
        prompt_text = f"""Look at the image and answer quickly.

Question: {real_question}

Answer: [Yes or No]
Reason: [One short sentence]"""
        
        prompt = [dict(type='text', value=prompt_text)]
        prompt.extend([dict(type='image', value=image_path)])
        response = model.generate(message=prompt)
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} å®Œæˆ")
    
    # è®°å½•ç¬¬ä¸€è½®
    debate_record['rounds'].append({
        'round': 1,
        'agent_1': agent_responses['agent_1'][0],
        'agent_2': agent_responses['agent_2'][0],
        'agent_3': agent_responses['agent_3'][0]
    })
    
    # ç¬¬äºŒè½®ï¼šå¿«é€Ÿå‚è€ƒå…¶ä»–ç­”æ¡ˆ
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šå¿«é€Ÿå†³ç­–")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} æœ€ç»ˆç­”è¾©ä¸­...")
        
        # åªè·å–å…¶ä»–agentçš„ç­”æ¡ˆï¼Œä¸åŒ…æ‹¬æ¨ç†è¿‡ç¨‹
        other_agents = [j for j in range(1, 4) if j != i]
        other_answers = [extract_answer(agent_responses[f'agent_{j}'][0]) for j in other_agents]
        other_summary = f"Others answered: {', '.join(other_answers)}"
        
        prompt_text = f"""Question: {real_question}

{other_summary}

Your final answer:
Answer: [Yes or No]
Reason: [Brief]"""
        
        prompt = [dict(type='text', value=prompt_text)]
        prompt.extend([dict(type='image', value=image_path)])
        response = model.generate(message=prompt)
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} ç­”è¾©å®Œæˆ")
    
    # è®°å½•ç¬¬äºŒè½®
    debate_record['rounds'].append({
        'round': 2,
        'agent_1': agent_responses['agent_1'][1],
        'agent_2': agent_responses['agent_2'][1],
        'agent_3': agent_responses['agent_3'][1]
    })
    
    # ç»Ÿè®¡ç¬¬äºŒè½®ç­”æ¡ˆç»“æœ
    print(f"\nğŸ“Š ç»Ÿè®¡æœ€ç»ˆç­”æ¡ˆ...")
    final_answers = []
    for i in range(1, 4):
        answer = extract_answer(agent_responses[f'agent_{i}'][1])
        final_answers.append(answer)
        print(f"  ğŸ“ Agent {i} ç­”æ¡ˆ: {answer}")
    
    # è®¡ç®—æœ€ç»ˆç­”æ¡ˆï¼ˆå¤šæ•°å†³ï¼‰
    answer_counter = Counter(final_answers)
    most_common = answer_counter.most_common(1)[0]
    final_answer = most_common[0]
    answer_count = most_common[1]
    
    print(f"  ğŸ“ˆ ç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    print(f"  ğŸ† æœ€ç»ˆç­”æ¡ˆ: {final_answer} (å‡ºç°: {answer_count}/3æ¬¡)")
    
    # è®°å½•ç»“æœ
    debate_record['final_decision'] = {
        'answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'final_answer': final_answer,
        'answer_count': answer_count
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': '3-Agent Fast Yes/No Debate',
        'final_answer': final_answer,
        'simple_answer': final_answer,
        'question': real_question,
        'ground_truth': base_answer,
        'answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'total_rounds': 2,
        'debate_concluded': True
    }
    
    print(f"\n{'ğŸ†'*40}")
    print(f"âš–ï¸ ä¸‰Agentå¿«é€Ÿè¾©è®ºç»“æŸ")
    print(f"ğŸ“ å„Agentç­”æ¡ˆ: {' '.join(final_answers)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    print(f"ğŸ“Š ç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    print(f"{'ğŸ†'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        debate_save_file = save_file.replace('.json', '_3agent_fast_debate.json')
        with open(debate_save_file, 'w', encoding='utf-8') as f:
            json.dump(debate_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ä¸‰Agentå¿«é€Ÿè¾©è®ºè®°å½•å·²ä¿å­˜åˆ°: {debate_save_file}")
    
    return debate_record, final_result

def baseline_self_refine_base(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒ - æµ‹è¯•è‡ªæˆ‘ä¼˜åŒ–CoTèƒ½åŠ›
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼šæ¨¡å‹ç»™å‡ºåˆå§‹ç­”æ¡ˆå’Œæ¨ç†
    2. ç¬¬äºŒè½®ï¼šæ¨¡å‹åŸºäºç¬¬ä¸€è½®ç»“æœè¿›è¡Œè‡ªæˆ‘åæ€å’Œä¼˜åŒ–
    3. ç¬¬ä¸‰è½®ï¼šæ¨¡å‹ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        refine_record: è‡ªæˆ‘ä¼˜åŒ–è®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nğŸ¤” å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒå¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    
    # è®°å½•
    refine_record = {
        'method': 'Single Model Self-Refine',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return response  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†")
    print(f"{'='*40}")
    
    prompt_text = f"""Look at the image and answer the question step by step.

Question: {real_question}

Please think through this step by step:
1. What do I see in the image?
2. What is the question asking?
3. Based on the image, what is my reasoning?
4. What is my answer?

Answer: [Yes or No]
Reasoning: [Your step-by-step reasoning]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_1 = model.generate(message=prompt)
    answer_1 = extract_answer(response_1)
    
    print(f"  ğŸ’­ åˆå§‹æ¨ç†å®Œæˆ")
    print(f"  ğŸ“ åˆå§‹ç­”æ¡ˆ: {answer_1}")
    
    # è®°å½•ç¬¬ä¸€è½®
    refine_record['rounds'].append({
        'round': 1,
        'response': response_1,
        'answer': answer_1
    })
    
    # ç¬¬äºŒè½®ï¼šè‡ªæˆ‘åæ€
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šè‡ªæˆ‘åæ€")
    print(f"{'='*40}")
    
    prompt_text = f"""Question: {real_question}

My previous reasoning and answer:
{response_1}

Now, let me reflect on my reasoning:
1. Was my initial analysis correct?
2. Did I miss any important details in the image?
3. Is my reasoning logical and complete?
4. Should I reconsider my answer?

Please provide your reflection and any corrections:
Reflection: [Your self-reflection]
Corrected Answer: [Yes or No]
Corrected Reasoning: [Updated reasoning if needed]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_2 = model.generate(message=prompt)
    answer_2 = extract_answer(response_2)
    
    print(f"  ğŸ’­ è‡ªæˆ‘åæ€å®Œæˆ")
    print(f"  ğŸ“ åæ€åç­”æ¡ˆ: {answer_2}")
    
    # è®°å½•ç¬¬äºŒè½®
    refine_record['rounds'].append({
        'round': 2,
        'response': response_2,
        'answer': answer_2
    })
    
    # ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç¡®è®¤
    print(f"\nğŸ¯ ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç¡®è®¤")
    print(f"{'='*40}")
    
    prompt_text = f"""Question: {real_question}

My initial reasoning: {response_1}
My reflection: {response_2}

Based on all my analysis, what is my final answer?

Final Answer: [Yes or No]
Final Reasoning: [Your conclusive reasoning]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_3 = model.generate(message=prompt)
    answer_3 = extract_answer(response_3)
    
    print(f"  ğŸ’­ æœ€ç»ˆç¡®è®¤å®Œæˆ")
    print(f"  ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {answer_3}")
    
    # è®°å½•ç¬¬ä¸‰è½®
    refine_record['rounds'].append({
        'round': 3,
        'response': response_3,
        'answer': answer_3
    })
    
    # åˆ†æç­”æ¡ˆå˜åŒ–
    answer_evolution = [answer_1, answer_2, answer_3]
    answer_changes = []
    for i in range(1, len(answer_evolution)):
        if answer_evolution[i] != answer_evolution[i-1]:
            answer_changes.append(f"Round {i}: {answer_evolution[i-1]} â†’ {answer_evolution[i]}")
    
    print(f"\nğŸ“Š ç­”æ¡ˆæ¼”åŒ–åˆ†æ...")
    print(f"  ğŸ“ ç­”æ¡ˆåºåˆ—: {' â†’ '.join(answer_evolution)}")
    if answer_changes:
        print(f"  ğŸ”„ ç­”æ¡ˆå˜åŒ–: {'; '.join(answer_changes)}")
    else:
        print(f"  âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    
    # è®°å½•ç»“æœ
    refine_record['final_decision'] = {
        'answer_evolution': answer_evolution,
        'answer_changes': answer_changes,
        'final_answer': answer_3,
        'total_rounds': 3
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': 'Single Model Self-Refine',
        'final_answer': answer_3,
        'simple_answer': answer_3,
        'question': real_question,
        'ground_truth': base_answer,
        'answer_evolution': answer_evolution,
        'answer_changes': answer_changes,
        'total_rounds': 3,
        'refine_concluded': True
    }
    
    print(f"\n{'ğŸ¤”'*40}")
    print(f"ğŸ¤” å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒç»“æŸ")
    print(f"ğŸ“ ç­”æ¡ˆæ¼”åŒ–: {' â†’ '.join(answer_evolution)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {answer_3}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    if answer_changes:
        print(f"ğŸ”„ ç­”æ¡ˆå˜åŒ–: {'; '.join(answer_changes)}")
    else:
        print(f"âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    print(f"{'ğŸ¤”'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        refine_save_file = save_file.replace('.json', '_self_refine.json')
        with open(refine_save_file, 'w', encoding='utf-8') as f:
            json.dump(refine_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Self-Refineè®°å½•å·²ä¿å­˜åˆ°: {refine_save_file}")
    
    return refine_record, final_result

def baseline_self_refine(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒ - ç®€åŒ–å¿«é€Ÿç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼šæ¨¡å‹ç»™å‡ºåˆå§‹ç­”æ¡ˆå’Œæ¨ç†
    2. ç¬¬äºŒè½®ï¼šæ¨¡å‹åŸºäºç¬¬ä¸€è½®ç»“æœè¿›è¡Œå¿«é€Ÿè‡ªæˆ‘åæ€
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        refine_record: è‡ªæˆ‘ä¼˜åŒ–è®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nğŸ¤” å•æ¨¡å‹Self-Refineå¿«é€Ÿç‰ˆæœ¬å¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    
    # è®°å½•
    refine_record = {
        'method': 'Single Model Self-Refine Fast',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return "YES"  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†")
    print(f"{'='*40}")
    
    prompt_text = f"""Look at the image and answer quickly.

Question: {real_question}

Answer: [Yes or No]
Reason: [Brief reasoning]"""
    
    response_1 = model.generate([image_path, prompt_text])
#     answer_1 = response_1
    answer_1 = extract_answer(response_1)
    
    print(f"  ğŸ’­ åˆå§‹æ¨ç†å®Œæˆ")
    print(f"  ğŸ“ åˆå§‹ç­”æ¡ˆ: {answer_1}")
    
    # è®°å½•ç¬¬ä¸€è½®
    refine_record['rounds'].append({
        'round': 1,
        'response': response_1,
        'answer': answer_1
    })
    
    # ç¬¬äºŒè½®ï¼šå¿«é€Ÿè‡ªæˆ‘åæ€
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šå¿«é€Ÿåæ€")
    print(f"{'='*40}")
    
    prompt_text = f"""Question: {real_question}

My previous answer: {answer_1}

Now, let me reflect on my reasoning:
1. Was my initial analysis correct?
2. Did I miss any important details in the image?
3. Is my reasoning logical and complete?
4. Should I reconsider my answer?

Final Answer: [Yes or No]
Brief reason: [Quick explanation]"""
    
    response_2 = model.generate([image_path, prompt_text])
    answer_2 = extract_answer(response_2)
#     answer_2 = response_1
    
    print(f"  ğŸ’­ å¿«é€Ÿåæ€å®Œæˆ")
    print(f"  ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {answer_2}")
    
    # è®°å½•ç¬¬äºŒè½®
    refine_record['rounds'].append({
        'round': 2,
        'response': response_2,
        'answer': answer_2
    })
    
    # åˆ†æç­”æ¡ˆå˜åŒ–
    answer_evolution = [answer_1, answer_2]
    answer_changed = answer_1 != answer_2
    
    print(f"\nğŸ“Š ç­”æ¡ˆåˆ†æ...")
    print(f"  ğŸ“ ç­”æ¡ˆåºåˆ—: {' â†’ '.join(answer_evolution)}")
    if answer_changed:
        print(f"  ğŸ”„ ç­”æ¡ˆå˜åŒ–: {answer_1} â†’ {answer_2}")
    else:
        print(f"  âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    
    # è®°å½•ç»“æœ
    refine_record['final_decision'] = {
        'answer_evolution': answer_evolution,
        'answer_changed': answer_changed,
        'final_answer': answer_2,
        'total_rounds': 2
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': 'Single Model Self-Refine Fast',
        'final_answer': answer_2,
        'simple_answer': answer_2,
        'question': real_question,
        'ground_truth': base_answer,
        'answer_evolution': answer_evolution,
        'answer_changed': answer_changed,
        'total_rounds': 2,
        'refine_concluded': True
    }
    
    print(f"\n{'ğŸ¤”'*40}")
    print(f"ğŸ¤” å•æ¨¡å‹Self-Refineå¿«é€Ÿç‰ˆæœ¬ç»“æŸ")
    print(f"ğŸ“ ç­”æ¡ˆæ¼”åŒ–: {' â†’ '.join(answer_evolution)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {answer_2}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    if answer_changed:
        print(f"ğŸ”„ ç­”æ¡ˆå˜åŒ–: {answer_1} â†’ {answer_2}")
    else:
        print(f"âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    print(f"{'ğŸ¤”'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        refine_save_file = save_file.replace('.json', '_self_refine_fast.json')
        with open(refine_save_file, 'w', encoding='utf-8') as f:
            json.dump(refine_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Self-Refineå¿«é€Ÿç‰ˆæœ¬è®°å½•å·²ä¿å­˜åˆ°: {refine_save_file}")
    
    return refine_record, final_result

def baseline_self_refine_intern(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒ - ç®€åŒ–å¿«é€Ÿç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼šæ¨¡å‹ç»™å‡ºåˆå§‹ç­”æ¡ˆå’Œæ¨ç†
    2. ç¬¬äºŒè½®ï¼šæ¨¡å‹åŸºäºç¬¬ä¸€è½®ç»“æœè¿›è¡Œå¿«é€Ÿè‡ªæˆ‘åæ€
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        refine_record: è‡ªæˆ‘ä¼˜åŒ–è®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nğŸ¤” å•æ¨¡å‹Self-Refineå¿«é€Ÿç‰ˆæœ¬å¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    
    # è®°å½•
    refine_record = {
        'method': 'Single Model Self-Refine Fast',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return "YES"  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†")
    print(f"{'='*40}")
    
    prompt_text = f"""Look at the image and answer quickly.

Question: {real_question}

Answer: [Yes or No]
Reason: [Brief reasoning]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_1 = model.generate(message=prompt)
    answer_1 = extract_answer(response_1)
    
    print(f"  ğŸ’­ åˆå§‹æ¨ç†å®Œæˆ")
    print(f"  ğŸ“ åˆå§‹ç­”æ¡ˆ: {answer_1}")
    
    # è®°å½•ç¬¬ä¸€è½®
    refine_record['rounds'].append({
        'round': 1,
        'response': response_1,
        'answer': answer_1
    })
    
    # ç¬¬äºŒè½®ï¼šå¿«é€Ÿè‡ªæˆ‘åæ€
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šå¿«é€Ÿåæ€")
    print(f"{'='*40}")
    
    prompt_text = f"""Question: {real_question}

My previous answer: {answer_1}

Quick reflection - should I change my answer?

Final Answer: [Yes or No]
Brief reason: [Quick explanation]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_2 = model.generate(message=prompt)
    answer_2 = extract_answer(response_2)
    
    print(f"  ğŸ’­ å¿«é€Ÿåæ€å®Œæˆ")
    print(f"  ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {answer_2}")
    
    # è®°å½•ç¬¬äºŒè½®
    refine_record['rounds'].append({
        'round': 2,
        'response': response_2,
        'answer': answer_2
    })
    
    # åˆ†æç­”æ¡ˆå˜åŒ–
    answer_evolution = [answer_1, answer_2]
    answer_changed = answer_1 != answer_2
    
    print(f"\nğŸ“Š ç­”æ¡ˆåˆ†æ...")
    print(f"  ğŸ“ ç­”æ¡ˆåºåˆ—: {' â†’ '.join(answer_evolution)}")
    if answer_changed:
        print(f"  ğŸ”„ ç­”æ¡ˆå˜åŒ–: {answer_1} â†’ {answer_2}")
    else:
        print(f"  âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    
    # è®°å½•ç»“æœ
    refine_record['final_decision'] = {
        'answer_evolution': answer_evolution,
        'answer_changed': answer_changed,
        'final_answer': answer_2,
        'total_rounds': 2
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': 'Single Model Self-Refine Fast',
        'final_answer': answer_2,
        'simple_answer': answer_2,
        'question': real_question,
        'ground_truth': base_answer,
        'answer_evolution': answer_evolution,
        'answer_changed': answer_changed,
        'total_rounds': 2,
        'refine_concluded': True
    }
    
    print(f"\n{'ğŸ¤”'*40}")
    print(f"ğŸ¤” å•æ¨¡å‹Self-Refineå¿«é€Ÿç‰ˆæœ¬ç»“æŸ")
    print(f"ğŸ“ ç­”æ¡ˆæ¼”åŒ–: {' â†’ '.join(answer_evolution)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {answer_2}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    if answer_changed:
        print(f"ğŸ”„ ç­”æ¡ˆå˜åŒ–: {answer_1} â†’ {answer_2}")
    else:
        print(f"âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    print(f"{'ğŸ¤”'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        refine_save_file = save_file.replace('.json', '_self_refine_fast.json')
        with open(refine_save_file, 'w', encoding='utf-8') as f:
            json.dump(refine_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Self-Refineå¿«é€Ÿç‰ˆæœ¬è®°å½•å·²ä¿å­˜åˆ°: {refine_save_file}")
    
    return refine_record, final_result

def baseline_self_refine_option(model, real_question, image_path, base_answer, options=["A", "B", "C", "D", "E"], save_file="", benchmark="MMMU"):
    """
    å•æ¨¡å‹Self-Refineæ¶ˆèå®éªŒ - æ”¯æŒé€‰é¡¹ç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼šæ¨¡å‹ç»™å‡ºåˆå§‹ç­”æ¡ˆå’Œæ¨ç†
    2. ç¬¬äºŒè½®ï¼šæ¨¡å‹åŸºäºç¬¬ä¸€è½®ç»“æœè¿›è¡Œè‡ªæˆ‘åæ€å’Œä¼˜åŒ–
    3. ç¬¬ä¸‰è½®ï¼šæ¨¡å‹ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ
        options: é€‰é¡¹åˆ—è¡¨ï¼Œå¦‚ ["A", "B", "C", "D"] æˆ– ["Yes", "No"]
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        refine_record: è‡ªæˆ‘ä¼˜åŒ–è®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nğŸ¤” å•æ¨¡å‹Self-Refineé€‰é¡¹ç‰ˆæœ¬å¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    if options:
        print(f"ğŸ“‹ é€‰é¡¹: {options}")
    
    import json
    import re
    
    # è®°å½•
    refine_record = {
        'method': 'Single Model Self-Refine with Options',
        'question': real_question,
        'ground_truth': base_answer,
        'options': options,
        'rounds': []
    }
    
    def extract_answer_with_options(response, options=None):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        if not options:
            # é»˜è®¤Yes/Noæ¨¡å¼
            patterns = [
                r'Answer:\s*(Yes|No)',
                r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
                r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
                r'\b(Yes|No)\b',
                r'\b(æ˜¯|å¦)\b'
            ]
            for pattern in patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    answer = match.group(1).upper()
                    if answer == "æ˜¯":
                        return "YES"
                    elif answer == "å¦":
                        return "NO"
                    else:
                        return answer
            return "YES"  # é»˜è®¤ç­”æ¡ˆ
        else:
            if "A" in response:
                return "A"
            elif "B" in response:
                return "B"
            elif "C" in response:
                return "C"
            elif "D" in response:
                return "D"
            elif "E" in response:
                return "E"
            else:
                return response
    
    # æ„å»ºé€‰é¡¹æ–‡æœ¬
    if options:
        options_text = "\n".join([f"{opt}. " for opt in options])
        answer_format = f"[{'/'.join(options)}]"
    else:
        options_text = ""
        answer_format = "[Yes or No]"
    
    # ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šåˆå§‹æ¨ç†")
    print(f"{'='*40}")
    
    prompt_text = f"""Look at the image and answer the question step by step.

{real_question}

Answer: {answer_format}
Reasoning: [Your step-by-step reasoning]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_1 = model.generate(message=prompt)
    answer_1 = extract_answer_with_options(response_1, options)
    
    print(f"  ğŸ’­ åˆå§‹æ¨ç†å®Œæˆ")
    print(f"  ğŸ“ åˆå§‹ç­”æ¡ˆ: {answer_1}")
    
    # è®°å½•ç¬¬ä¸€è½®
    refine_record['rounds'].append({
        'round': 1,
        'response': response_1,
        'answer': answer_1
    })
    
    # ç¬¬äºŒè½®ï¼šè‡ªæˆ‘åæ€
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šè‡ªæˆ‘åæ€")
    print(f"{'='*40}")
    
    prompt_text = f"""{real_question}

My previous reasoning and answer:
{response_1}

Quick reflection - should I change my answer?

Please provide your reflection and any corrections:
Reflection: [Your self-reflection]
Corrected Answer: {answer_format}
Corrected Reasoning: [Updated reasoning if needed]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_2 = model.generate(message=prompt)
    answer_2 = extract_answer_with_options(response_2, options)
    
    print(f"  ğŸ’­ è‡ªæˆ‘åæ€å®Œæˆ")
    print(f"  ğŸ“ åæ€åç­”æ¡ˆ: {answer_2}")
    
    # è®°å½•ç¬¬äºŒè½®
    refine_record['rounds'].append({
        'round': 2,
        'response': response_2,
        'answer': answer_2
    })
    
    # ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç¡®è®¤
    print(f"\nğŸ¯ ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç¡®è®¤")
    print(f"{'='*40}")
    
    prompt_text = f"""{real_question}

My initial reasoning: {response_1}
My reflection: {response_2}

Based on all my analysis, what is my final answer?

Final Answer: {answer_format}
Final Reasoning: [Your conclusive reasoning]"""
    
    prompt = [dict(type='text', value=prompt_text)]
    prompt.extend([dict(type='image', value=image_path)])
    response_3 = model.generate(message=prompt)
    answer_3 = extract_answer_with_options(response_3, options)
    
    print(f"  ğŸ’­ æœ€ç»ˆç¡®è®¤å®Œæˆ")
    print(f"  ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {answer_3}")
    
    # è®°å½•ç¬¬ä¸‰è½®
    refine_record['rounds'].append({
        'round': 3,
        'response': response_3,
        'answer': answer_3
    })
    
    # åˆ†æç­”æ¡ˆå˜åŒ–
    answer_evolution = [answer_1, answer_2, answer_3]
    answer_changes = []
    for i in range(1, len(answer_evolution)):
        if answer_evolution[i] != answer_evolution[i-1]:
            answer_changes.append(f"Round {i}: {answer_evolution[i-1]} â†’ {answer_evolution[i]}")
    
    print(f"\nğŸ“Š ç­”æ¡ˆæ¼”åŒ–åˆ†æ...")
    print(f"  ğŸ“ ç­”æ¡ˆåºåˆ—: {' â†’ '.join(answer_evolution)}")
    if answer_changes:
        print(f"  ğŸ”„ ç­”æ¡ˆå˜åŒ–: {'; '.join(answer_changes)}")
    else:
        print(f"  âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    
    # è®°å½•ç»“æœ
    refine_record['final_decision'] = {
        'answer_evolution': answer_evolution,
        'answer_changes': answer_changes,
        'final_answer': answer_3,
        'total_rounds': 3
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': 'Single Model Self-Refine with Options',
        'final_answer': response_3,
        'simple_answer': answer_3,
        'question': real_question,
        'ground_truth': base_answer,
        'options': options,
        'answer_evolution': answer_evolution,
        'answer_changes': answer_changes,
        'total_rounds': 3,
        'refine_concluded': True
    }
    
    print(f"\n{'ğŸ¤”'*40}")
    print(f"ğŸ¤” å•æ¨¡å‹Self-Refineé€‰é¡¹ç‰ˆæœ¬ç»“æŸ")
    print(f"ğŸ“ ç­”æ¡ˆæ¼”åŒ–: {' â†’ '.join(answer_evolution)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {answer_3}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    if answer_changes:
        print(f"ğŸ”„ ç­”æ¡ˆå˜åŒ–: {'; '.join(answer_changes)}")
    else:
        print(f"âœ… ç­”æ¡ˆç¨³å®š: æ— å˜åŒ–")
    print(f"{'ğŸ¤”'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        refine_save_file = save_file.replace('.json', '_self_refine_options.json')
        with open(refine_save_file, 'w', encoding='utf-8') as f:
            json.dump(refine_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Self-Refineé€‰é¡¹ç‰ˆæœ¬è®°å½•å·²ä¿å­˜åˆ°: {refine_save_file}")
    
    return refine_record, final_result


def baseline_mad_debate_maj(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    ä¸‰ä¸ªagentè¾©è®º2è½® + å¤šæ•°å†³å®š (Yes/Noé—®é¢˜)
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼š3ä¸ªagentç‹¬ç«‹åˆ†æ
    2. ç¬¬äºŒè½®ï¼š3ä¸ªagentçœ‹åˆ°å…¶ä»–agentç¬¬ä¸€è½®ç­”æ¡ˆåç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    3. å¤šæ•°å†³ï¼šä»ç¬¬äºŒè½®ç­”æ¡ˆä¸­æå–Yes/Noé€‰é¡¹ï¼Œé€‰æ‹©æœ€å¤šçš„ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜ (Yes/Noé—®é¢˜)
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ (Yes/No)
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        debate_record: è¾©è®ºè®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nâš”ï¸ ä¸‰Agent Yes/Noè¾©è®ºç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    from collections import Counter
    
    # è®°å½•
    debate_record = {
        'method': '3-Agent Yes/No Debate',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    # å­˜å‚¨æ¯è½®ç­”æ¡ˆ
    agent_responses = {
        'agent_1': [],
        'agent_2': [], 
        'agent_3': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return "YES"  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šç‹¬ç«‹åˆ†æ
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šç‹¬ç«‹åˆ†æ")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} åˆ†æä¸­...")
        prompt_text = f"""Analyze this image and answer the question.

Question: {real_question}

Format:
Answer: [Yes or No]
Reasoning: [Brief explanation]"""
        
#         prompt = [dict(type='text', value=prompt_text)]
#         prompt.extend([dict(type='image', value=image_path)])
#         response = model.generate(message=prompt)
        response = model.generate([image_path, prompt_text])
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} å®Œæˆ")
    
    # è®°å½•ç¬¬ä¸€è½®
    debate_record['rounds'].append({
        'round': 1,
        'agent_1': agent_responses['agent_1'][0],
        'agent_2': agent_responses['agent_2'][0],
        'agent_3': agent_responses['agent_3'][0]
    })
    
    # ç¬¬äºŒè½®ï¼šæœ€ç»ˆç­”è¾©è½®ï¼ˆå‚è€ƒå…¶ä»–æ„è§åç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼‰
    print(f"\nğŸ¯ ç¬¬äºŒè½®ï¼šæœ€ç»ˆç­”è¾©")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} æœ€ç»ˆç­”è¾©ä¸­...")
        
        # è·å–å…¶ä»–ä¸¤ä¸ªagentçš„ç¬¬ä¸€è½®ç­”æ¡ˆ
        other_agents = [j for j in range(1, 4) if j != i]
        other_responses = "\n".join([f"Agent {j}: {agent_responses[f'agent_{j}'][0]}" for j in other_agents])
        
        prompt_text = f"""Question: {real_question}

Other analysts' opinions from round 1:
{other_responses}

This is the final round. Consider the above opinions and provide your final answer:
Answer: [Yes or No]
Reasoning: [Brief explanation]"""
        
#         prompt = [dict(type='text', value=prompt_text)]
#         prompt.extend([dict(type='image', value=image_path)])
#         response = model.generate(message=prompt)
        response = model.generate([image_path, prompt_text])
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} ç­”è¾©å®Œæˆ")
    
    # è®°å½•ç¬¬äºŒè½®
    debate_record['rounds'].append({
        'round': 2,
        'agent_1': agent_responses['agent_1'][1],
        'agent_2': agent_responses['agent_2'][1],
        'agent_3': agent_responses['agent_3'][1]
    })
    
    # ç»Ÿè®¡ç¬¬äºŒè½®ç­”æ¡ˆç»“æœ
    print(f"\nğŸ“Š ç»Ÿè®¡æœ€ç»ˆç­”æ¡ˆ...")
    final_answers = []
    for i in range(1, 4):
        answer = extract_answer(agent_responses[f'agent_{i}'][1])
        final_answers.append(answer)
        print(f"  ğŸ“ Agent {i} ç­”æ¡ˆ: {answer}")
    
    # è®¡ç®—æœ€ç»ˆç­”æ¡ˆï¼ˆå¤šæ•°å†³ï¼‰
    answer_counter = Counter(final_answers)
    most_common = answer_counter.most_common(1)[0]
    final_answer = most_common[0]
    answer_count = most_common[1]
    
    print(f"  ğŸ“ˆ ç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    print(f"  ğŸ† æœ€ç»ˆç­”æ¡ˆ: {final_answer} (å‡ºç°: {answer_count}/3æ¬¡)")
    
    # è®°å½•ç»“æœ
    debate_record['final_decision'] = {
        'answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'final_answer': final_answer,
        'answer_count': answer_count
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': '3-Agent Yes/No Debate',
        'final_answer': final_answer,
        'simple_answer': final_answer,
        'question': real_question,
        'ground_truth': base_answer,
        'answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'total_rounds': 2,
        'debate_concluded': True
    }
    
    print(f"\n{'ğŸ†'*40}")
    print(f"âš–ï¸ ä¸‰Agent Yes/Noè¾©è®ºç»“æŸ")
    print(f"ğŸ“ å„Agentç­”æ¡ˆ: {' '.join(final_answers)}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    print(f"ğŸ“Š ç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    print(f"{'ğŸ†'*40}")
    
    # ä¿å­˜è®°å½•
    if save_file:
        debate_save_file = save_file.replace('.json', '_3agent_yesno_debate.json')
        with open(debate_save_file, 'w', encoding='utf-8') as f:
            json.dump(debate_record, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ä¸‰Agent Yes/Noè¾©è®ºè®°å½•å·²ä¿å­˜åˆ°: {debate_save_file}")
    
    return debate_record, final_result


def baseline_mad_debate(model, real_question, image_path, base_answer, save_file="", benchmark="MMMU"):
    """
    ä¸‰ä¸ªagentè¾©è®º3è½® + æ³•å®˜å†³ç­– (Yes/Noé—®é¢˜) - ç®€åŒ–ç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. ç¬¬ä¸€è½®ï¼š3ä¸ªagentå¿«é€Ÿåˆ†æ
    2. ç¬¬äºŒè½®ï¼š3ä¸ªagentç®€å•å‚è€ƒå…¶ä»–æ„è§
    3. ç¬¬ä¸‰è½®ï¼š3ä¸ªagentç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    4. æ³•å®˜å†³ç­–ï¼šæ³•å®˜ç®€å•é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆ
    
    Args:
        model: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
        real_question: åŸå§‹é—®é¢˜ (Yes/Noé—®é¢˜)
        image_path: å›¾åƒè·¯å¾„
        base_answer: æ­£ç¡®ç­”æ¡ˆ (Yes/No)
        save_file: ä¿å­˜æ–‡ä»¶è·¯å¾„
        benchmark: æ•°æ®é›†åç§°
    
    Returns:
        debate_record: è¾©è®ºè®°å½•
        final_result: æœ€ç»ˆç»“æœ
    """
    print(f"\nâš”ï¸ ä¸‰Agent+æ³•å®˜ Yes/Noè¾©è®ºç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“ é—®é¢˜: {real_question}")
    print(f"ğŸ¯ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    
    import json
    import re
    from collections import Counter
    
    # è®°å½•
    debate_record = {
        'method': '3-Agent Yes/No Debate with Judge',
        'question': real_question,
        'ground_truth': base_answer,
        'rounds': []
    }
    
    # å­˜å‚¨æ¯è½®ç­”æ¡ˆ
    agent_responses = {
        'agent_1': [],
        'agent_2': [], 
        'agent_3': []
    }
    
    def extract_answer(response):
        """ä»å›å¤ä¸­æå–ç­”æ¡ˆé€‰é¡¹"""
        patterns = [
            r'Answer:\s*(Yes|No)',
            r'ç­”æ¡ˆ[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'é€‰æ‹©[ï¼š:]\s*(æ˜¯|å¦|Yes|No)',
            r'\b(Yes|No)\b',
            r'\b(æ˜¯|å¦)\b'
        ]
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                answer = match.group(1).upper()
                # å¤„ç†ä¸­æ–‡ç­”æ¡ˆ
                if answer == "æ˜¯":
                    return "YES"
                elif answer == "å¦":
                    return "NO"
                else:
                    return answer
        return "YES"  # é»˜è®¤ç­”æ¡ˆ
    
    # ç¬¬ä¸€è½®ï¼šå¿«é€Ÿåˆ†æ
    print(f"\nğŸ¯ ç¬¬ä¸€è½®ï¼šå¿«é€Ÿåˆ†æ")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} åˆ†æä¸­...")
        prompt_text = f"""Look at this image and answer quickly.

Question: {real_question}

Answer: [Yes or No]
Reason: [One sentence only]"""
        
        prompt = [dict(type='text', value=prompt_text)]
        prompt.extend([dict(type='image', value=image_path)])
        response = model.generate(message=prompt)
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} å®Œæˆ")
    
    # è®°å½•ç¬¬ä¸€è½®
    debate_record['rounds'].append({
        'round': 1,
        'agent_1': agent_responses['agent_1'][0],
        'agent_2': agent_responses['agent_2'][0],
        'agent_3': agent_responses['agent_3'][0]
    })
    
    # ç¬¬äºŒè½®ï¼šç®€å•å‚è€ƒ
    print(f"\nğŸ”¥ ç¬¬äºŒè½®ï¼šç®€å•å‚è€ƒ")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} åˆ†æä¸­...")
        
        # è·å–å…¶ä»–ä¸¤ä¸ªagentçš„ç­”æ¡ˆï¼ˆä»…ç­”æ¡ˆï¼Œä¸åŒ…æ‹¬æ¨ç†ï¼‰
        other_agents = [j for j in range(1, 4) if j != i]
        other_answers = [extract_answer(agent_responses[f'agent_{j}'][0]) for j in other_agents]
        other_summary = f"Others said: {', '.join(other_answers)}"
        
        prompt_text = f"""Question: {real_question}

{other_summary}

Your answer:
Answer: [Yes or No]
Reason: [Brief]"""
        
        prompt = [dict(type='text', value=prompt_text)]
        prompt.extend([dict(type='image', value=image_path)])
        response = model.generate(message=prompt)
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} å®Œæˆ")
    
    # è®°å½•ç¬¬äºŒè½®
    debate_record['rounds'].append({
        'round': 2,
        'agent_1': agent_responses['agent_1'][1],
        'agent_2': agent_responses['agent_2'][1],
        'agent_3': agent_responses['agent_3'][1]
    })
    
    # ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç­”æ¡ˆ
    print(f"\nğŸ¯ ç¬¬ä¸‰è½®ï¼šæœ€ç»ˆç­”æ¡ˆ")
    print(f"{'='*40}")
    
    for i in range(1, 4):
        print(f"  ğŸ’­ Agent {i} æœ€ç»ˆç­”è¾©ä¸­...")
        
        prompt_text = f"""Question: {real_question}

Give your final answer:
Answer: [Yes or No]
Reason: [Short]"""
        
        prompt = [dict(type='text', value=prompt_text)]
        prompt.extend([dict(type='image', value=image_path)])
        response = model.generate(message=prompt)
        agent_responses[f'agent_{i}'].append(response)
        print(f"  âœ… Agent {i} ç­”è¾©å®Œæˆ")
    
    # è®°å½•ç¬¬ä¸‰è½®
    debate_record['rounds'].append({
        'round': 3,
        'agent_1': agent_responses['agent_1'][2],
        'agent_2': agent_responses['agent_2'][2],
        'agent_3': agent_responses['agent_3'][2]
    })
    
    # æ”¶é›†ç¬¬ä¸‰è½®ç­”æ¡ˆ
    print(f"\nğŸ“Š æ”¶é›†å„Agentæœ€ç»ˆç­”æ¡ˆ...")
    final_answers = []
    agent_final_responses = []
    for i in range(1, 4):
        answer = extract_answer(agent_responses[f'agent_{i}'][2])
        final_answers.append(answer)
        agent_final_responses.append(agent_responses[f'agent_{i}'][2])
        print(f"  ğŸ“ Agent {i} ç­”æ¡ˆ: {answer}")
    
    # æ³•å®˜å¿«é€Ÿå†³ç­–
    print(f"\nâš–ï¸ æ³•å®˜å¿«é€Ÿå†³ç­–")
    print(f"{'='*40}")
    print(f"  ğŸ‘¨â€âš–ï¸ æ³•å®˜å¿«é€Ÿé€‰æ‹©...")
    
    # ç®€åŒ–çš„æ³•å®˜prompt
    agent_answers_only = [f"Agent {i+1}: {final_answers[i]}" for i in range(3)]
    
    judge_prompt_text = f"""Question: {real_question}

Three answers: {' | '.join(agent_answers_only)}

Pick one answer quickly:
Selected Answer: [Yes or No]
Chosen Agent: [Agent 1, Agent 2, or Agent 3]
Why: [One sentence]"""
    
    judge_prompt = [dict(type='text', value=judge_prompt_text)]
    judge_prompt.extend([dict(type='image', value=image_path)])
    judge_response = model.generate(message=judge_prompt)
    
    # æå–æ³•å®˜çš„å†³å®š
    final_answer = extract_answer(judge_response)
    
    # ç¡®å®šè¢«é€‰ä¸­çš„agent
    chosen_agent_match = re.search(r'Chosen Agent:\s*Agent\s*([123])', judge_response, re.IGNORECASE)
    chosen_agent = chosen_agent_match.group(1) if chosen_agent_match else "1"
    
    print(f"  âœ… æ³•å®˜è¯„å®¡å®Œæˆ")
    print(f"  ğŸ† æ³•å®˜é€‰æ‹©: Agent {chosen_agent}")
    print(f"  ğŸ“‹ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿ç•™ç”¨äºåˆ†æï¼‰
    answer_counter = Counter(final_answers)
    print(f"  ğŸ“ˆ å„Agentç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    
    # è®°å½•æ³•å®˜è½®æ¬¡
    debate_record['judge_round'] = {
        'judge_response': judge_response,
        'chosen_agent': chosen_agent,
        'final_decision': final_answer
    }
    
    # è®°å½•ç»“æœ
    debate_record['final_decision'] = {
        'agent_answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'judge_response': judge_response,
        'chosen_agent': chosen_agent,
        'final_answer': final_answer,
        'decision_method': 'judge_selection'
    }
    
    # æœ€ç»ˆç»“æœ
    final_result = {
        'method': '3-Agent Yes/No Debate with Judge',
        'final_answer': final_answer,
        'simple_answer': final_answer,
        'question': real_question,
        'ground_truth': base_answer,
        'agent_answers': final_answers,
        'answer_distribution': dict(answer_counter),
        'chosen_agent': chosen_agent,
        'judge_response': judge_response,
        'total_rounds': 3,
        'decision_method': 'judge_selection',
        'debate_concluded': True
    }
    
    print(f"\n{'ğŸ†'*40}")
    print(f"âš–ï¸ ä¸‰Agent Yes/Noè¾©è®º+æ³•å®˜å†³ç­–ç»“æŸ")
    print(f"ğŸ“ å„Agentç­”æ¡ˆ: {' '.join(final_answers)}")
    print(f"ğŸ‘¨â€âš–ï¸ æ³•å®˜é€‰æ‹©: Agent {chosen_agent}")
    print(f"ğŸ’¬ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    print(f"ğŸ’¬ æ­£ç¡®ç­”æ¡ˆ: {base_answer}")
    print(f"ğŸ“Š ç­”æ¡ˆç»Ÿè®¡: {dict(answer_counter)}")
    print(f"{'ğŸ†'*40}")
    
    # ä¿å­˜è®°å½•
#     if save_file:
#         debate_save_file = save_file.replace('.json', '_3agent_judge_debate.json')
#         with open(debate_save_file, 'w', encoding='utf-8') as f:
#             json.dump(debate_record, f, ensure_ascii=False, indent=2)
#         print(f"\nğŸ’¾ ä¸‰Agent+æ³•å®˜å†³ç­–è®°å½•å·²ä¿å­˜åˆ°: {debate_save_file}")
    
    return debate_record, final_result