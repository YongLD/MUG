<div align="center">

<h1>
    <img src="assets/logo.png" alt="Logo" width="40" style="vertical-align: middle;" /> 
    <b>Multi-agent Undercover Gaming</b>: Hallucination Removal via Counterfactual Test for Multimodal Reasoning
  </h1>

[Dayong Liang](https://github.com/YongLD)<sup>1,3,\*</sup>, [Xiao-Yong Wei](https://scholar.google.com/citations?user=8kxWTokAAAAJ&hl=en)<sup>2,\*</sup>, [Changmeng Zheng](https://github.com/thecharm)<sup>2,‚Ä†</sup>

<p><sup>1</sup>South China University of Technology &nbsp;&nbsp;<sup>2</sup>The Hong Kong Polytechnic &nbsp;&nbsp;<sup>3</sup>Peng Cheng Laboratory
<br><sup>*</sup>Contributed equally to this work &nbsp;&nbsp; <sup>‚Ä†</sup>Corresponding author
<h5 align="center">

<!-- [![arXiv](https://img.shields.io/badge/Arxiv-2406.07476-AD1C18.svg?logo=arXiv)](https://arxiv.org/pdf/2403.14972) -->

</h5>

<!-- <img src="assets/mug.png" alt="framework" title="MUG" width="700" /> -->

</div>

<!-- The MUG protocol introduces a novel approach to mitigate hallucinations in large language models through systematic counterfactual validation. By leveraging principles from social deduction games, our framework enhances reasoning reliability among multi-agent systems. -->

---

*Latest News* üî•

- üîÑ **Code Implementation**: In progress
- ‚úÖ [2025/11] **Paper Accepted**: Our paper has been accepted at AAAI2026! üéâ

## Features

- Systematic counterfactual testing for hallucination detection
- Dynamic cross-evidence reasoning capabilities
- Active reasoning paradigms facilitating exploratory dialogue among agents

## ‚ù§Ô∏è Acknowledgments

- [VLMEvalKit](https://github.com/open-compass/VLMEvalKit): An open-source evaluation toolkit of large vision-language models (LVLMs).

<!-- ## üìë Citation

If this repo is useful to you, please cite using this BibTeX.
```bibtex

``` -->
